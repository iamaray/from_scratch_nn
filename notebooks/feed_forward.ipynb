{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully-Connected Feed Forward Neural Network\n",
    "Here, we implement the 'simplest' neural network architecture, a *fully-connected neural network*, wherein a particular node of a layer has a weight contribution (i.e. a *connection*) from every node of the previous layer. Here is a visual representation:\n",
    "\n",
    "![](FCNN.png)\n",
    "\n",
    "## Layers\n",
    "The first layer is the *input layer*. In our training set $\\mathcal T = \\{ (x_1, y_1), (x_2, y_2), \\cdots \\}$, this layer takes the $x_i$ inputs. The last layer is the *output layer*, which produces the model output. Between the input and output layers are the *hidden layers*, which perform the bulk of the computation and contain the model parameters ($W$ for weights and $B$ for biases).\n",
    "\n",
    "### Layer Inputs\n",
    "Let $l_{k,j}$ be node $j$ of layer $k$. As previously stated, this node takes a weight contribution from every node of the previous layer (as well as a single bias term). Hence, the input to $l_{k,j}$ may be expressed as $$\\operatorname{in}(l_{k,j}) = [b_k \\quad w_{k-1, 1, j} \\quad w_{k-1, 2, j} \\quad \\cdots \\quad w_{k-1, d_{k-1}, j}] \\space \\mathbf{x}_{k-1} = \\mathbf{w}_{k-1,j} \\cdot \\mathbf{x}_{k-1},$$ where $w_{k-1, i, j}$ denotes the weight contribution from node $i$ of layer $k-1$ to node $j$ of layer $k$, and $\\mathbf{x}_{k-1}$ is the vector of outputs from the previous layer (with a $1$ prepended to account for the bias term). This simple construction allows us to express a whole layer's inputs as a matrix multiplication: $$ in_k = L^T_k \\mathbf{x}_{k - 1}, \\qquad L_k = \\begin{bmatrix} \\mathbf{b}_{k-1} & \\mathbf{w}_{k-1, 1} & \\mathbf{w}_{k-1, 2} & \\cdots & \\mathbf{w}_{k-1, d_{k-1}} \\end{bmatrix}$$\n",
    "\n",
    "### Layer Outputs\n",
    "Now, after the layer inputs are calculuated, we apply some sort of element-wise non-linearity $g : \\mathbb R^{d_{k-1}} \\rightarrow \\mathbb R^{d_{k-1}}$. Hence, $out_k := g(in_k)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, layer_dim, layer_bias, layer_weights):\n",
    "        self.dim = layer_dim\n",
    "        self.bias = layer_bias\n",
    "        self.inputs = []\n",
    "        self.outputs = []\n",
    "        \n",
    "    def computeInputs():\n",
    "        pass\n",
    "    \n",
    "    def computeOutputs():\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
